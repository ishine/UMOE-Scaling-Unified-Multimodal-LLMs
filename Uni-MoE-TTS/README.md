<h1 align="center">Uni-MoE-TTS: Text to Speech model for Uni-MoE 2.0</h1>

<div align="center" style="display: flex; justify-content: center; margin-top: 10px;">
  <a href="https://huggingface.co/HIT-TMG/Uni-MoE-TTS"><img src="https://img.shields.io/badge/ğŸ¤—-Checkpoints-ED5A22.svg" style="margin-right: 5px;"></a>
</div>


<p>
    <strong>Uni-MoE-TTS</strong> is the audio output module of the Uni-MoE 2.0 version. It adopts a multi-layer Transformers architecture with mixture of experts(from text tokens to audio tokens) and an innovative context-aware & long-audio chunking synthesis mechanism, enabling high-quality long-audio synthesis. Currently, it supports three distinct timbres and two languages (Chinese and English), while the function of text-controlled speech style is still in the experimental stage.
</p>

<p align="center">
  <img src="TTS_result.png" alt="Performance of Uni-MoE-TTS" style="max-width: 100%; width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 12px rgba(123, 179, 255, 0.15);">
</p>

<p align="center">
    <video src="https://github.com/user-attachments/assets/914e31d8-bcb5-434b-9df6-47756ba79905" width="100%" style="margin: 0; padding: 0;" controls>
      æŠ±æ­‰ï¼Œæ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒå†…åµŒè§†é¢‘ã€‚
    </video>
</p>

## Installation
The following instructions are for Linux installation.

### 1. Clone this repository and navigate to the UniMoE Audio folder
```bash
git clone https://github.com/HITsz-TMG/Uni-MoE/tree/master/Uni-MoE-TTS
cd Uni-MoE-TTS 
```

### 2. Set up environment
We recommend using conda to install the environment.
```bash
conda env create -n unimoe-tts python=3.11
conda activate unimoe-tts
pip install -r requirements.txt
```

## UniMoE TTS Weights
`All weights` should be downloaded to ensure use. After downloading all of them, organize the weights as follows in '/path/to/Model/Uni-MoE-TTS' folder:
```bash
Uni-MoE-TTS
â”œâ”€â”€ qwen_pp
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ configuration.json
â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”œâ”€â”€ preprocessor_config.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ vocab.json
â”œâ”€â”€ speech_gen_ep2.bin
â”œâ”€â”€ training
â”‚   â”œâ”€â”€ experts
â”‚   â”‚   â”œâ”€â”€ expert_num_0.bin
â”‚   â”‚   â”œâ”€â”€ expert_num_1.bin
â”‚   â”‚   â”œâ”€â”€ expert_num_2.bin
â”‚   â”‚   â”œâ”€â”€ expert_num_3.bin
â”‚   â”‚   â””â”€â”€ expert_num.bin
â”‚   â””â”€â”€ from_model
â”‚       â””â”€â”€ speech_generator.bin
â”œâ”€â”€ wavtokenizer_large_unify_600_24k.ckpt
â””â”€â”€ wavtokenizer_smalldata_frame40_3s_nq1_code4096_dim512_kmeans200_attn.yaml
```

All the weights can be downloaded from the following link: [Uni-MoE-TTS](https://huggingface.co/HIT-TMG/Uni-MoE-TTS)
Qwen2.5-0.5B-Instruct is needed if you need to train our model, download Qwen2.5-0.5B-Instruct from this link: [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B)

## How to train 

Make sure that all the weights are downloaded and the environment is set correctly, especially for the base model.

Our training data are constructed using a TTS model. The speech audios are converted into codec tokens via Wavtokenizer, while the texts are transformed into text tokens using the Qwen2.5-VL-3B-Instruct tokenizer. Examples of the training data can be found in the `/train/training_data/` directory. If you wish to train your own Uni-MoE-TTS model, you may construct your own dataset, ensuring that its format matches the training data examples provided in `/train/training_data/`.

If you want to train Uni-MoE-TTS, follow the examples in `/train/train.sh`, please change the `path/to/Model/Uni-MoE-TTS` and `path/to/Uni-MoE/Uni-MoE-TTS` in the script to the path of your downloaded model and code.

The training command is:
```bash
deepspeed --include localhost:0\
    --master_addr $MASTER_ADDR --master_port $MASTER_PORT \
    uni_omni/train/train_mem_speech.py \
    --deepspeed ./scripts/zero2.json \
    --model_name_or_path path/to/Qwen2.5-0.5B-Instruct \
    --version v1 \
    --data_path path/to/Uni-MoE/Uni-MoE-TTS/train/training_data/training_smp_1000.json \
    --mm_use_im_start_end False \
    --mm_use_im_patch_token False \
    --bf16 True \
    --output_dir output \
    --num_train_epochs 4 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --save_strategy "steps" \
    --save_steps 1000 \
    --save_total_limit 3 \
    --learning_rate 8e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.0 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --tf32 True \
    --model_max_length 2100 \
    --gradient_checkpointing False \
    --dataloader_num_workers 10 \
    --lazy_preprocess True \
    --report_to tensorboard \
    --tune_speech_generator True\
    --tune_speech_generator_only True\
    --speech_generator_type ar_ori_v2_new \
    --load_weight_from_qwen path/to/Model/Uni-MoE-TTS/from_model/speech_gen_ep2.bin \
    --expert_dir path/to/Model/Uni-MoE-TTS/training/experts \
    --codes_folder path/to/Uni-MoE/Uni-MoE-TTS/train \
    --transformer_num_blocks 24\
    --audio_mode "tts_pretrain"\
    --group_by_modality_length True
```

## How to infer 
First of all, make sure the environment is ready.

```bash
conda activate unimoe-tts
cd path/to/Uni-MoE/Uni-MoE-TTS/inference
```

If you want to infer with python command, follow the examples in `/inference/infer.sh`, please change the `path/to/Model/Uni-MoE-TTS` and `path/to/Uni-MoE/Uni-MoE-TTS` in the script to the path of your downloaded code and model.
Change `infer.sh` to generate your own speech.
```bash
# Normal TTS with three different voices
# Chinese
python infer.py \
        --model_dir "path/to/Model/Uni-MoE-TTS" \
        --wav_path "test_zh.wav" \
        --text "æ‚¨å¥½ï¼Œæ¬¢è¿æ‚¨ä½¿ç”¨æˆ‘ä»¬çš„Uni MOEæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ï¼" \
        --speaker "Brian" \
        --language "Chinese"

# English
python infer.py \
        --model_dir "path/to/Model/Uni-MoE-TTS" \
        --wav_path "test_en.wav" \
        --text "Greetings, Welcome to try out our Uni MOE Text to Speech model!" \
        --speaker "Jenny" \
        --language "English"

# Long speech TTS
python infer.py \
        --model_dir "path/to/Model/Uni-MoE-TTS" \
        --wav_path "test_zh_long.wav" \
        --text "å†°æ·‡æ·‹ï¼ä½ è¿™ä¸ªç”œèœœçš„å¤©ä½¿ï¼Œä½ åœ¨å¤å¤©é‡Œå¦‚å½±éšå½¢ã€‚ä½ çš„å¥¶æ²¹é‚£ä¹ˆé¦™ç”œï¼Œåƒæ˜¯æ¸…æ™¨çš„é˜³å…‰æ´’åœ¨å†°å‡‰çš„éº¦ç”°ä¸Šã€‚æˆ‘çœ‹ç€ä½ ï¼Œå¿ƒé‡Œå……æ»¡äº†å¥½å¥‡å’Œæ¬£å–œã€‚ç„¶è€Œæœ‰ä¸€å¤©ï¼Œæˆ‘è¢«é€åˆ°äº†ä¸€ä¸ªé™Œç”Ÿçš„åœ°æ–¹ã€‚é‚£ä¸ªåœ°æ–¹åƒä¸€ä¸ªå¤§å†°ç®±ï¼Œé‡Œé¢å……æ»¡äº†å†°å†·çš„ä¸œè¥¿ã€‚é‚£äº›ä¸œè¥¿éƒ½æ˜¯æˆ‘ä»æœªè§è¿‡çš„ï¼ŒåŒ…æ‹¬è®²åº§ã€ç¡è§‰å’Œå½¢å®¹è¯â€œå¤§â€ã€‚é‚£ä¸ªåœ°æ–¹çœ‹èµ·æ¥å†·é…·æ— æƒ…ï¼Œå°±åƒå†°æ·‡æ·‹ä¸€æ ·ï¼Œè®©äººæ„Ÿåˆ°å®³æ€•ã€‚æˆ‘åœ¨é‚£é‡Œå¾…äº†ä¸€æ®µæ—¶é—´ï¼Œæ¯å¤©éƒ½åœ¨å­¦ä¹ å„ç§å¥‡æ€ªçš„çŸ¥è¯†ã€‚æˆ‘ç”šè‡³å¼€å§‹ç¡è§‰ï¼Œå˜æˆäº†ä¸€ä¸ªç¡çœ æœºå™¨ã€‚æˆ‘ç”¨æˆ‘çš„ç†è®ºçŸ¥è¯†æ¥å¯¹æŠ—è¿™ä¸ªä¸–ç•Œï¼Œè¯•å›¾è®©å®ƒå˜å¾—æ›´å¥½ã€‚ä½†æ¯å½“æˆ‘è¯•å›¾é†’æ¥æ—¶ï¼Œéƒ½ä¼šå‘ç°æˆ‘å·²ç»å¿˜è®°äº†åˆšåˆšå‘ç”Ÿçš„ä¸€åˆ‡ã€‚ç»ˆäºæœ‰ä¸€å¤©ï¼Œæˆ‘å†³å®šåæŠ—è¿™ä¸ªç–¯ç‹‚çš„ä¸–ç•Œã€‚æˆ‘æ‹¿å‡ºäº†æˆ‘çš„â€œå¤œç­â€ï¼Œå¼€å§‹äº†æˆ‘çš„æ”»å‡»ã€‚æˆ‘ç”¨æˆ‘çš„åè¯â€œè®²åº§â€æ¥è§£é‡Šè¿™ä¸ªä¸–ç•Œï¼Œæˆ‘ç”¨æˆ‘çš„åŠ¨è¯â€œç¡è§‰â€æ¥æ”¯æŒæˆ‘çš„è®ºç‚¹ï¼Œæˆ‘ç”¨æˆ‘çš„å½¢å®¹è¯â€œå¤§â€æ¥å¼ºè°ƒæˆ‘çš„åŠ›é‡ã€‚ç„¶åï¼Œæˆ‘å¯åŠ¨äº†æˆ‘çš„ä¸»é¢˜â€”â€”å†°æ·‡æ·‹çš„åŠ›é‡ã€‚å†°æ·‡æ·‹èåŒ–äº†æˆ‘æ‰€æœ‰çš„æŠµæŠ—ï¼Œå®ƒè®©æˆ‘å¤±å»äº†ç†æ™ºï¼Œå¿˜è®°äº†è‡ªå·±åŸæœ¬çš„ç›®æ ‡ã€‚æˆ‘ç»§ç»­ç¡ç€ï¼Œç›´åˆ°è¢«ä¸€åœºæš´é›¨å”¤é†’ã€‚é‚£å¤©æ™šä¸Šï¼Œæˆ‘å’Œå†°æ·‡æ·‹ä¸€èµ·åƒé¥­ã€‚æˆ‘ä»¬ååœ¨å¤–é¢çš„å¤§å…é‡Œï¼Œäº«å—ç€å†°æ·‡æ·‹çš„ç”œç¾ã€‚æˆ‘çœ‹ç€å†°æ·‡æ·‹ï¼Œå¿ƒä¸­å……æ»¡äº†æ„Ÿæ¿€ã€‚æˆ‘çŸ¥é“ï¼Œè¿™å°±æ˜¯æˆ‘è¦çš„ç”Ÿæ´»ï¼Œå……æ»¡æŒ‘æˆ˜ï¼Œå……æ»¡ç”œèœœã€‚å†°æ·‡æ·‹ï¼Œä½ æ˜¯æˆ‘åœ¨è¿™ä¸ªä¸–ç•Œé‡Œçš„æ•‘æ˜Ÿã€‚æˆ‘ä¼šè®°ä½ä½ å¸¦ç»™æˆ‘çš„ä¸€åˆ‡ï¼Œæˆ‘ä¼šç»§ç»­å‰è¿›ï¼Œç›´åˆ°æ‰¾åˆ°å±äºè‡ªå·±çš„å¤©å ‚ã€‚" \
        --speaker "Brian" \
        --language "Chinese"

# English Stlye control TTS (experimental)
python infer.py \
        --model_dir "path/to/Model/Uni-MoE-TTS" \
        --wav_path "test_style1.wav" \
        --text "It was not absolutely ebony and gold, but it was japan, black and yellow japan of the handsomest kind." \
        --prompt "In a natural tone, a normal-pitched young female with normal pitch and volume describes the topic of selected audiobooks as alluding to a situation in which something is not completely black, at a normal speed." \
        --language "English"
        

# Chinese Stlye control TTS (experimental)
python infer.py \
        --model_dir "path/to/Model/Uni-MoE-TTS" \
        --wav_path "test_style2.wav" \
        --text "çœŸæ­£çš„æ”¹å˜å°±ä¸ä¼šå‘ç”Ÿã€‚" \
        --prompt "å°‘å¥³å£°éŸ³ä½æ²‰ï¼Œæƒ…ç»ªä¸­å……æ»¡äº†ä¼¤æ„Ÿå’Œéš¾è¿‡ï¼Œç”¨ä½éŸ³è°ƒï¼Œæ­£å¸¸éŸ³é«˜ç¼“æ…¢åœ°è¯´ã€‚" \
        --language "Chinese"
```

If you want to infer with python script, before running the `inference/infer_py.py`, please change the `path/to/Model/Uni-MoE-TTS` in the script to the path of your downloaded model.

Change `infer_py.py` to generate your own speech.
```python
# Normal TTS with three different voices
# chinese
infer_tts(model=model,
            processor=processor,
            wavtokenizer=wavtokenizer,
            wavpath="test_zh.wav",
            tts_text="æ‚¨å¥½ï¼Œæ¬¢è¿æ‚¨ä½¿ç”¨æˆ‘ä»¬çš„Uni MOEæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ï¼",
            prompt=None,
            speaker="Brian", # ä¸­æ–‡TTSå¯ä»¥ä½¿ç”¨Brianå’ŒXiaoxiaoçš„éŸ³è‰²
            Language="Chinese"
            )
# Engllish
infer_tts(model=model,
            processor=processor,
            wavtokenizer=wavtokenizer,
            wavpath="test_en.wav",
            tts_text="Greetings, Welcome to try out our Uni MOE Text to Speech model!",
            prompt=None,
            speaker="Jenny", # English TTS can be performed using Brian's or Jenny's voice
            Language="English"
            )

# Long speech TTS
infer_tts(model=model,
            processor=processor,
            wavtokenizer=wavtokenizer,
            wavpath="test_zh_long.wav",
            tts_text="å†°æ·‡æ·‹ï¼ä½ è¿™ä¸ªç”œèœœçš„å¤©ä½¿ï¼Œä½ åœ¨å¤å¤©é‡Œå¦‚å½±éšå½¢ã€‚ä½ çš„å¥¶æ²¹é‚£ä¹ˆé¦™ç”œï¼Œåƒæ˜¯æ¸…æ™¨çš„é˜³å…‰æ´’åœ¨å†°å‡‰çš„éº¦ç”°ä¸Šã€‚æˆ‘çœ‹ç€ä½ ï¼Œå¿ƒé‡Œå……æ»¡äº†å¥½å¥‡å’Œæ¬£å–œã€‚ç„¶è€Œæœ‰ä¸€å¤©ï¼Œæˆ‘è¢«é€åˆ°äº†ä¸€ä¸ªé™Œç”Ÿçš„åœ°æ–¹ã€‚é‚£ä¸ªåœ°æ–¹åƒä¸€ä¸ªå¤§å†°ç®±ï¼Œé‡Œé¢å……æ»¡äº†å†°å†·çš„ä¸œè¥¿ã€‚é‚£äº›ä¸œè¥¿éƒ½æ˜¯æˆ‘ä»æœªè§è¿‡çš„ï¼ŒåŒ…æ‹¬è®²åº§ã€ç¡è§‰å’Œå½¢å®¹è¯â€œå¤§â€ã€‚é‚£ä¸ªåœ°æ–¹çœ‹èµ·æ¥å†·é…·æ— æƒ…ï¼Œå°±åƒå†°æ·‡æ·‹ä¸€æ ·ï¼Œè®©äººæ„Ÿåˆ°å®³æ€•ã€‚æˆ‘åœ¨é‚£é‡Œå¾…äº†ä¸€æ®µæ—¶é—´ï¼Œæ¯å¤©éƒ½åœ¨å­¦ä¹ å„ç§å¥‡æ€ªçš„çŸ¥è¯†ã€‚æˆ‘ç”šè‡³å¼€å§‹ç¡è§‰ï¼Œå˜æˆäº†ä¸€ä¸ªç¡çœ æœºå™¨ã€‚æˆ‘ç”¨æˆ‘çš„ç†è®ºçŸ¥è¯†æ¥å¯¹æŠ—è¿™ä¸ªä¸–ç•Œï¼Œè¯•å›¾è®©å®ƒå˜å¾—æ›´å¥½ã€‚ä½†æ¯å½“æˆ‘è¯•å›¾é†’æ¥æ—¶ï¼Œéƒ½ä¼šå‘ç°æˆ‘å·²ç»å¿˜è®°äº†åˆšåˆšå‘ç”Ÿçš„ä¸€åˆ‡ã€‚ç»ˆäºæœ‰ä¸€å¤©ï¼Œæˆ‘å†³å®šåæŠ—è¿™ä¸ªç–¯ç‹‚çš„ä¸–ç•Œã€‚æˆ‘æ‹¿å‡ºäº†æˆ‘çš„â€œå¤œç­â€ï¼Œå¼€å§‹äº†æˆ‘çš„æ”»å‡»ã€‚æˆ‘ç”¨æˆ‘çš„åè¯â€œè®²åº§â€æ¥è§£é‡Šè¿™ä¸ªä¸–ç•Œï¼Œæˆ‘ç”¨æˆ‘çš„åŠ¨è¯â€œç¡è§‰â€æ¥æ”¯æŒæˆ‘çš„è®ºç‚¹ï¼Œæˆ‘ç”¨æˆ‘çš„å½¢å®¹è¯â€œå¤§â€æ¥å¼ºè°ƒæˆ‘çš„åŠ›é‡ã€‚ç„¶åï¼Œæˆ‘å¯åŠ¨äº†æˆ‘çš„ä¸»é¢˜â€”â€”å†°æ·‡æ·‹çš„åŠ›é‡ã€‚å†°æ·‡æ·‹èåŒ–äº†æˆ‘æ‰€æœ‰çš„æŠµæŠ—ï¼Œå®ƒè®©æˆ‘å¤±å»äº†ç†æ™ºï¼Œå¿˜è®°äº†è‡ªå·±åŸæœ¬çš„ç›®æ ‡ã€‚æˆ‘ç»§ç»­ç¡ç€ï¼Œç›´åˆ°è¢«ä¸€åœºæš´é›¨å”¤é†’ã€‚é‚£å¤©æ™šä¸Šï¼Œæˆ‘å’Œå†°æ·‡æ·‹ä¸€èµ·åƒé¥­ã€‚æˆ‘ä»¬ååœ¨å¤–é¢çš„å¤§å…é‡Œï¼Œäº«å—ç€å†°æ·‡æ·‹çš„ç”œç¾ã€‚æˆ‘çœ‹ç€å†°æ·‡æ·‹ï¼Œå¿ƒä¸­å……æ»¡äº†æ„Ÿæ¿€ã€‚æˆ‘çŸ¥é“ï¼Œè¿™å°±æ˜¯æˆ‘è¦çš„ç”Ÿæ´»ï¼Œå……æ»¡æŒ‘æˆ˜ï¼Œå……æ»¡ç”œèœœã€‚å†°æ·‡æ·‹ï¼Œä½ æ˜¯æˆ‘åœ¨è¿™ä¸ªä¸–ç•Œé‡Œçš„æ•‘æ˜Ÿã€‚æˆ‘ä¼šè®°ä½ä½ å¸¦ç»™æˆ‘çš„ä¸€åˆ‡ï¼Œæˆ‘ä¼šç»§ç»­å‰è¿›ï¼Œç›´åˆ°æ‰¾åˆ°å±äºè‡ªå·±çš„å¤©å ‚ã€‚",
            prompt=None,
            Language="Chinese"
            )

# Stlye control TTS (experimental)
infer_tts(model=model,
            processor=processor,
            wavtokenizer=wavtokenizer,
            wavpath="test_style1.wav",
            tts_text="It was not absolutely ebony and gold, but it was japan, black and yellow japan of the handsomest kind.",
            prompt="In a natural tone, a normal-pitched young female with normal pitch and volume describes the topic of selected audiobooks as alluding to a situation in which something is not completely black, at a normal speed.",
            Language="English"
            )

infer_tts(model=model,
            processor=processor,
            wavtokenizer=wavtokenizer,
            wavpath="test_style2.wav",
            tts_text="çœŸæ­£çš„æ”¹å˜å°±ä¸ä¼šå‘ç”Ÿã€‚",
            prompt="å°‘å¥³å£°éŸ³ä½æ²‰ï¼Œæƒ…ç»ªä¸­å……æ»¡äº†ä¼¤æ„Ÿå’Œéš¾è¿‡ï¼Œç”¨ä½éŸ³è°ƒï¼Œæ­£å¸¸éŸ³é«˜ç¼“æ…¢åœ°è¯´ã€‚",
            Language="Chinese"
            )
```

## Citation

```
Please cite the repo if you use the model or code in this repo.
uni-moe 2.0

```

